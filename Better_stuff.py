# -*- coding: utf-8 -*-
"""Predict Employee Attrition [Coloured Cow].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EIhoALfsL16tdc65KEYpxedemCY-oSy3

##Welcome to this Colab notebook, Prateek. Over the next few minutes, I'll be running a model over a pre-cleaned dataset to predict employee attrition. Regardless on how it performs on the training/testing data, I'll define a function so that we can try it out on some real data and perhaps predict if a ColouredCow © employee is likely to remain or stay.

## Before creating any model, let me lay the process of the model building method.

1.   Import the libraries
2.   Load the dataset
3.   Check for missing values in the dataset
4.   Infer some basic statistical details (mean, median, min, max, etc.)
5.   Visualize and create subplots of relevent features
6.   Understand object types in the dataset and remove irrelevent columns
7.   Get correlations of the dataset and visualize it using a seaborn heatmap
8.   Preprocess dataset to create an ML model.
9.   Apply  different classifiers on the cleaned, preprocessed, and ready to train dataset
10.  Get accuracy, confusion matrix, and other relevent metrics of the models
11.  Choose the best performing model and understand the most important features using a feature importance.
12.  Define a function to input parameters of ColouredCow © employees and predict if they are likely to remain or leave

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

1.   Import the libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

"""  2. Load the dataset: 
     (Note to self; Upload the dataset before running the notebook!)
"""

df = pd.read_csv("C:/Users/Sukant Sidnhwani/Desktop/Codes n stuffs/ML Practise/1 - Datasets/HR-Employee-Attrition.csv")
df.head(5) # View the first 5 rows

# Get the number of rows and columns in the dateset
print(f"This dataset has {df.shape[1]} data points on {df.shape[0]} employees.")

"""  3. Check for missing values in the dataset

"""

print(f"Number of missing values in this dataset: {df.isna().sum().sum()}")

"""  4. Infer some basic statistical details (mean, median, min, max, etc.)"""

df.describe()

"""  5. Visualize and create subplots of relevent features"""

# See how many customers end up leaving and how many end up staying
sns.histplot(df['Attrition'], shrink = 0.95, fill = False);

# Visualizing the same with age added as a feature

fig, ax = plt.subplots(figsize = (18, 7)) # Making the plot wide so we can see all the bins
sns.set_theme(style = 'darkgrid')
sns.countplot(x = 'Age', hue = 'Attrition', palette = "Set3", data = df, ax = ax, edgecolor = sns.color_palette("dark", 1));

"""According to the histogram, we can tell that employees aged 28-31 are most likely to leave. Not many people over the age of 35 leave.

------------------------------------------------------------------------

6. Understand object types in the dataset and remove irrelevent columns

  Now let's see some of the unique features with unique values and figure out which one of these are actually useful for the model. We'll only consider the object datatypes as their unique categories are limited. Integer datatypes (Such as hourly wage, number of hours, etc.) have infinite number of unique values so we'll leave that out.
"""

for i in df.columns:
  if df[i].dtype == object:
    print(str(i) + ': ' + str(df[i].unique()))
    print(df[i].value_counts())
    print("__________________________________________________________________")

"""Something to note: 

  1. The variable, 'Over18' only has 1 unique value so it's pretty useless. We should remove that.
  2. Having a second look at the data, we can see that the variable, 'Standard Hours' also has one value i.e. 80. We can remove that as well.
  3. For our ML model we should also remove the features that include wages as this dataset is skewed towards employees in the US and most likely does not reflect the wages at ColouredCow ©. I think it's wise to leave them out. It might result in a loss of accuracy but it'll still work.
  4. Also removing 'StockOptionLevel' as the company hasn't gone public (yet).
"""

# Removing useless variables
del df['Over18'] # Contains only one value
del df['StandardHours'] # Contains only one value 
del df['DailyRate'] # Not including wages as they might cause the model to be biased
del df['MonthlyIncome'] # Not including wages as they might cause the model to be biased
del df['MonthlyRate'] # Not including wages as they might cause the model to be biased
del df['EmployeeCount'] # Contains only one value
del df['HourlyRate'] # Not including wages as they might cause the model to be biased
del df['StockOptionLevel'] # Company not public yet

# Viewing df.head(5) again
df.head(5)

"""Before creating the ML models, we need to wrangle the dataset slightly so that the attrition column is the first column and the independent variables all follow it. To do that, we'll create a new Age column and put it at the end of the df and then delete the original column. Here's what the dataset should look like when we're done."""

df['Age_Column'] = df['Age']
del df['Age']
del df['EmployeeNumber'] # Nobody cares 

df.head(5)

"""8. Preprocess dataset to create an ML model. 
     The first thing we'll do is encode the labels into numbers. For example, for the Attrition column, 'Yes' will be 1 and 'No will be 0.
"""

import warnings
warnings.filterwarnings('ignore') # To ignore deprecation warnings. Very unpleasing to look at. 

le = LabelEncoder()

for column in df.columns:
  i = 0
  if df[column].dtype == np.number:
    continue
  else:
    df[column] = le.fit_transform(df[column])

df.head(5) # Just to see how the encoded dataset looks

"""  7. Get correlations of the dataset and visualize it using a seaborn heatmap"""

corr = df.corr()
print(corr)

"""Visualizing the correlations in a better way using Seaborn Heatmapping"""

plt.figure(figsize = (20, 20))
sns.heatmap(df.corr(), annot = True, fmt = '.0%');
nice_columns = corr[abs(corr['Attrition']) > 0.10]['Attrition']
print(nice_columns)

"""The heatmap is pretty self explanatory. Some obvious points are:
1. There's a 78% positive correlation between Job Level and Total Working years. Pretty easy to decipher that the more working years one has, the higher his/her level is.
2. There's a 0% correlation between Environment Satisfaction and Years spent working under the current manager. Makes sense as these two are completely unrelated.
3. Performance rating has a 77% positive correlation with Percantage Salary Hike. Makes perfect sense as the higher your rating is, the more likely one is to receive a hike. 

____________________________________________________________________________________"""

"""Now we need to split the dataset into independant variables (X) and dependant variable (y). After that we need to transforming the dataset into a training & testing set. After that, we need to scale the dataset so that all the values are between -1 & +1. This is known as normalization and its important for many ML models."""

# Splitting df into X & y

# Creating a condition to select only those columns that have a correlation of > 10% or less than -10% with the target variable (Attrition)
filters = [(df['Attrition'].corr(df[col])) > 0.10 or (df['Attrition'].corr(df[col])) < -0.10 for col in df.columns]

concat_df = df.iloc[:, filters] # Creating a concatenated df to refer to later

X = df.iloc[:, filters].values[:, 1:] # Removing attrition column from X
y = df.iloc[:, 0].values

# Splitting the dataset into training & testing values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training, 30% testing

# Scaling the values using Standard Scaler

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""# Trying varios ML models to see which one works the best"""

# Fitting a logistic regression model 
log_cl = LogisticRegression(max_iter = 10000).fit(X_train, y_train)
y_pred1 = log_cl.predict(X_test)

ac1 = accuracy_score(y_test, y_pred1)

# Fitting a K nearest neighbours model 

# Default method i.e. manually choose the number of neighbours 
# knn_cl = KNeighborsClassifier(n_neighbors = 15, metric = 'minkowski', p = 2).fit(X_train, y_train)
# y_pred2 = knn_cl.predict(X_test)

# ac2 = accuracy_score(y_test, y_pred2)

# Iterative method i.e. run a loop of different neighbours and choose the best one 
scorelist = []
kindex = []

for i, neighbours in enumerate(range(1, 25)):
    knn_cl = KNeighborsClassifier(n_neighbors = neighbours)
    knn_cl.fit(X_train, y_train)
    y_pred2 = knn_cl.predict(X_test)
    ac2 = accuracy_score(y_test, y_pred2)
    scorelist.append(ac2)
    kindex.append(i)
       
ac2 = sorted(scorelist, reverse = True)[:1]
ac2 = float(np.array(ac2))
kindex = sorted(kindex, reverse = True)[:1]
kindex = int(np.array(kindex))

knn_cl = KNeighborsClassifier(n_neighbors = kindex).fit(X_train, y_train)

# Fitting Support Vector Machine model [Linear]
svm_cl = SVC(kernel = 'linear').fit(X_train, y_train)
y_pred3 = svm_cl.predict(X_test)

ac3 = accuracy_score(y_test, y_pred3)

# Fitting Support Vector Machine model [Gaussian]
svmg_cl = SVC(kernel = 'rbf').fit(X_train, y_train)
y_pred4 = svmg_cl.predict(X_test)

ac4 = accuracy_score(y_test, y_pred4)

# Fitting Naive Bayes Model
nb_cl = GaussianNB().fit(X_train, y_train)
y_pred5 = nb_cl.predict(X_test)

ac5 = accuracy_score(y_test, y_pred5)

# Fitting a Decision tree model 
dt_cl = DecisionTreeClassifier(criterion = 'entropy').fit(X_train, y_train)
y_pred6 = dt_cl.predict(X_test)

ac6 = accuracy_score(y_test, y_pred6)

# Fitting a Random Forrest model with an iterative loop 
rflist = []
rfindex = []

for i, trees in enumerate(range(10, 200)):
    rf_cl = RandomForestClassifier(n_estimators = trees, criterion = 'entropy')
    rf_cl.fit(X_train, y_train)
    y_pred7 = rf_cl.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred7)
    rflist.append(rf_accuracy)
    rfindex.append(i)
       
ac7 = sorted(rflist, reverse = True)[:1]
ac7 = float(np.array(ac7))
rindex = sorted(rfindex, reverse = True)[:1]
rindex = int(np.array(rindex))

rf_cl = RandomForestClassifier(n_estimators = rindex, criterion = 'entropy').fit(X_train, y_train)
    
# Fitting a Gradient Boost with an iterative loop 
gblist = []
gbindex = []

for i, estimators in enumerate(range(5, 200)):
    gb_cl = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = estimators)
    gb_cl.fit(X_train, y_train)
    y_pred8 = gb_cl.predict(X_test)
    gb_accuracy = accuracy_score(y_test, y_pred8)
    gblist.append(gb_accuracy)
    gbindex.append(i)
    
ac8 = sorted(gblist, reverse = True)[:1]
ac8 = float(np.float64(ac8))
gbindex = sorted(gbindex, reverse = True)[:1]
gbindex = int(np.array(gbindex))

gb_cl = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = gbindex).fit(X_train, y_train)

# Evaluating all accuracies into a list
list1 = [ac1, ac2, ac3, ac4, ac5, ac6, ac7, ac8]
list1 = [i * 100 for i in list1]
list1 = [round(num, 2) for num in list1]

classification = pd.DataFrame()
classification['Classification Models'] = ['Logistic Regression Classification', 
                                            'K Nearest Neighbors Classification', 
                                            'Support Vector Machine [Linear] Classification', 
                                            'Support Vector Machine [Gaussian] Classification',
                                            'Naive Bayes Classification',
                                            'Decision Tree Classification', 
                                            'Random Forrest Classification',
                                            'Gradient Boosting Classification']

classification['Accuracies'] = list1
print(classification)

"""According to this dataset, Naive Bayes performs the best as it's accuracy is 88.44%. We'll perform the demo predictions on this model."""

""" Now we need to create a function that inputs values from user and returns demo prediction"""

print("Please refer to the following legends and enter the relevent values.\n\nEnvironment Satisfaction: 1-4 where 1 denotes low environment satisfaction and 4 denotes high environment satisfaction.\n\nJob Involvement: 1-4 where 1 denotes Low involvement and 4 denotes High involvement.\n\nJob Level: 1-5 where 1 denotes Entry Level and 5 denotes Senior Level.\n\nJob Satisfaction: 1-4 where  denotes lowest job satisfaction and 4 denotes high job satisfaction.\n\nMarital Status: 0 for Divorced, 1 for Married, 2 for Single.\n\nOvertime: If the employee regularly works overtime, 1 for Yes 0 for No.\n\nTotal Working Years: Number of years the employee has worked. (Including ColoredCow)\n\nYears at Company: Number of years spent at ColoredCow.\n\nYears in Current Role: Number of years spent at employee's current role.\n\nYears with Current Manager: Number of years spent under current Manager. (Enter Median Value 3 if unsure)\n\nAge: Age of the employee.")

demo = []

while True:
    vals = [int(input()) for i in range(concat_df.columns[1:])]
    demo.append(vals)
    

# while True:
#     for column in concat_df.columns[1:]:
#         vals = input(f"{column}?: ")
#         vals = [int(input()) for i in range(concat_df.columns[1:])]
#         if vals.lower() == "exit" or vals.lower() == "quit":
#             break
#         break
#     break


# while True:
#     demo = input(r"Enter a sentence: ")
#     # demo = demo.lower()
#     if demo.lower() == "exit" or demo.lower() == "quit":
#         break
#     demo = re.sub('[^a-zA-Z]', ' ', str(demo))
#     demo = demo.split()
#     demo = [ps.stem(word) for word in demo if not word in stop_words]
#     demo = ' '.join(demo)
#     demo = [demo]
#     demo_text = cv.transform(demo).toarray()
#     demo_pred = rf.predict(demo_text)
#     if demo_pred == ['1']:
#         print("Sarcasm detected.")
#     else:
#         print("Sarcasm not detected.") 
#     # demo = []
        
# ## Saving the classification report to a .csv file
# # classification.to_csv('Models.csv')
